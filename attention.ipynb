{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRpwwBpk+HLI1a24rUJ6gc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngtuetam/attention_mechanism/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC-DZ1oi-YyQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from random import randint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsmT29E0-hWs",
        "outputId": "4cf32e7b-2b5c-4cc8-eed9-4d0061da7bb2"
      },
      "source": [
        "! curl --silent -L -o data.zip \"https://drive.google.com/uc?export=download&id=1d6eUqRstk7NIpyASzbuIsDvBdHEwfU0g\"\n",
        "! unzip -q data.zip\n",
        "! ls data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data.csv  human_vocab.json  machine_vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHyCAOjW-3DS",
        "outputId": "dfb555e6-dc17-4d57-a2a4-60327f1cda17"
      },
      "source": [
        "\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    X = df[0].values\n",
        "    y = df[1].values\n",
        "    x_tok = Tokenizer(char_level=True, filters='')\n",
        "    x_tok.fit_on_texts(X)\n",
        "    y_tok = Tokenizer(char_level=True, filters='')\n",
        "    y_tok.fit_on_texts(y)\n",
        "    \n",
        "    X = x_tok.texts_to_sequences(X)\n",
        "    y = y_tok.texts_to_sequences(y)\n",
        "    \n",
        "    X = pad_sequences(X)\n",
        "    y = np.asarray(y)\n",
        "    \n",
        "    return X, y, x_tok.word_index, y_tok.word_index\n",
        "\n",
        "X, y, x_wid, y_wid= load_data('data/data.csv')\n",
        "x_id2w = dict(zip(x_wid.values(), x_wid.keys()))\n",
        "y_id2w = dict(zip(y_wid.values(), y_wid.keys()))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "print('train size: {} - test size: {}'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 18750 - test size: 6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pFsLxkKQNYg",
        "outputId": "4d2b69bb-29a3-40b5-cb46-cc6ee179fc5c"
      },
      "source": [
        "# hidden size cho môt hình LSTM\n",
        "hidden_size = 128\n",
        "learning_rate = 0.001\n",
        "decoder_learning_ratio = 0.1\n",
        "\n",
        "# tập tự vựng của các câu đầu vào \n",
        "# +1 vì các bạn cần kí tự padding nhé!\n",
        "input_size = len(x_wid) + 1\n",
        "\n",
        "# +2 vì các bạn cần kí tự bắt đầu và kết thức\n",
        "output_size = len(y_wid) + 2\n",
        "# 2 kí tự này nằm ở cuối\n",
        "sos_idx = len(y_wid) \n",
        "eos_idx = len(y_wid) + 1\n",
        "\n",
        "max_length = y.shape[1]\n",
        "print(\"input vocab: {} - output vocab: {} - length of target: {}\".format(input_size, output_size, max_length))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input vocab: 35 - output vocab: 13 - length of target: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjbQ5q2MQR4k"
      },
      "source": [
        "def decoder_sentence(idxs, vocab):\n",
        "    text = ''.join([vocab[w] for w in idxs if (w > 0) and (w in vocab)])\n",
        "    return text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtcbTxuWQXD8"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # embedding vector của từ\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # mô hình GRU biến thể RNN để học vector biểu diễn của câu\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # input: SxB        \n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden # SxBxH, 1xBxH              \n",
        "\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attn ,self).__init__()\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        ### Mô hình nhận trạng thái hidden hiện tại của mô hình decoder, \n",
        "        ### và các hidden states của mô hình encoder\n",
        "        # encoder_outputs: TxBxH\n",
        "        # hidden: SxBxH\n",
        "        \n",
        "        # tranpose về đúng shape để nhận ma trận\n",
        "        encoder_outputs = torch.transpose(encoder_outputs, 0, 1) #BxTxH\n",
        "        hidden = torch.transpose(torch.transpose(hidden, 0, 1), 1, 2) # BxHxS\n",
        "        # tính e, chính là tương tác giữ hidden và các trạng thái ẩn của mô hình encoder \n",
        "        energies = torch.bmm(encoder_outputs, hidden) # BxTxS\n",
        "        energies = torch.transpose(energies, 1, 2) # BxSxT\n",
        "        # tính alpha, chính là trọng số của trung bình có trọng số cần tính bằng hàm softmax\n",
        "        attn_weights = F.softmax(energies, dim=-1) #BxSxT\n",
        "        \n",
        "        # tính context vector bằng trung binh có trọng số\n",
        "        output = torch.bmm(attn_weights, encoder_outputs) # BxSxH\n",
        "        \n",
        "        # trả về chiều cần thiết\n",
        "        output = torch.transpose(output, 0, 1) # SxBxH\n",
        "        attn_weights = torch.transpose(attn_weights, 0, 1) #SxBxT\n",
        "        \n",
        "        # return context vector và các trọng số alpha cho mục đích biểu diễn cơ chế attention\n",
        "        return output, attn_weights\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # vector biểu diễn cho các từ của output\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        # định nghĩa mô hình attention ở trên\n",
        "        self.attn = Attn(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # mô hình decoder là GRU\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        \n",
        "        # dự đoán các từ tại mội thời điểm, chúng ta nối 2 vector hidden và context lại với nhau \n",
        "        self.concat = nn.Linear(self.hidden_size*2, hidden_size)        \n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input: SxB\n",
        "        # encoder_outputs: BxSxH\n",
        "        # hidden: 1xBxH\n",
        "        embedded = self.embedding(input) # 1xBxH\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        # biểu diễn của câu\n",
        "        rnn_output, hidden = self.gru(embedded, hidden)  #SxBxH, 1xBxH\n",
        "        # tính context vector dựa trên các hidden states\n",
        "        context, attn_weights = self.attn(rnn_output, encoder_outputs) # SxBxH\n",
        "        \n",
        "        # nối hidden state của mô hình decoder hiện tại và context vector để dự đoán \n",
        "        concat_input = torch.cat((rnn_output, context), -1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input)) #SxBxH\n",
        "        \n",
        "        # dự đoán kết quả tại mỗi thời điểm\n",
        "        output = self.out(concat_output) # SxBxoutput_size\n",
        "        return output, hidden, attn_weights"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F037aR-PQdEc"
      },
      "source": [
        "encoder = Encoder(input_size, hidden_size)\n",
        "decoder = Decoder(output_size, hidden_size, 0.1)\n",
        "\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "input_encoder = torch.randint(1, input_size, (34, 6), dtype=torch.long)\n",
        "encoder_outputs, hidden = encoder(input_encoder)\n",
        "input_decoder = torch.randint(1, output_size, (10, 6), dtype=torch.long)\n",
        "output, hidden, attn_weights = decoder(input_decoder, hidden, encoder_outputs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTZ1f6cmQitO"
      },
      "source": [
        "def forward_and_compute_loss(inputs, targets, encoder, decoder, criterion):\n",
        "    batch_size = inputs.size()[1]\n",
        "    \n",
        "    # định nghĩa 2 kí tự bắt đầu và kết thúc\n",
        "    sos = Variable(torch.ones((1, batch_size), dtype=torch.long)*sos_idx)\n",
        "    eos = Variable(torch.ones((1, batch_size), dtype=torch.long)*eos_idx)\n",
        "    \n",
        "    # input của mô hình decoder phải thêm kí tự bắt đầu\n",
        "    decoder_inputs = torch.cat((sos, targets), dim=0)\n",
        "    # output cần dự đoán của mô hình decoder phải thêm kí tự kết thúc\n",
        "    decoder_targets = torch.cat((targets, eos), dim=0)\n",
        "    \n",
        "    # forward tính hidden states của câu\n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "    # tính output của mô hình decoder\n",
        "    output, hidden, attn_weights = decoder(decoder_inputs, encoder_hidden, encoder_outputs)\n",
        "    \n",
        "    output = torch.transpose(torch.transpose(output, 0, 1), 1, 2) # BxCxS\n",
        "    decoder_targets = torch.transpose(decoder_targets, 0, 1)\n",
        "    # tính loss \n",
        "    loss = criterion(output, decoder_targets)\n",
        "    \n",
        "    return loss, output\n",
        "\n",
        "def train(inputs, targets,  encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    # khai báo train để mô hình biết là đang train hay test\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    # zero gradient, phải làm mỗi khi cập nhất gradient\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    \n",
        "    # tính loss dựa vào hàm đã định nghĩa ở trên\n",
        "    train_loss, output = forward_and_compute_loss(inputs, targets,encoder, decoder,criterion)    \n",
        "    \n",
        "    train_loss.backward()\n",
        "    # cập nhật một step\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    # return loss để print :D\n",
        "    return train_loss.item()\n",
        "\n",
        "def evaluate(inputs, targets, encoder, decoder, criterion):\n",
        "    # báo cho mô hình biết đang test/eval\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    # tính loss\n",
        "    eval_loss, output = forward_and_compute_loss(inputs, targets, encoder, decoder,criterion)\n",
        "    output = torch.transpose(output, 1, 2)\n",
        "    # dự đoán của mỗi thời điểm các vị trí có prob lớn nhất\n",
        "    pred_idx = torch.argmax(output, dim=-1).squeeze(-1)\n",
        "    pred_idx = pred_idx.data.cpu().numpy()\n",
        "    \n",
        "    # return loss và kết quả dự đoán\n",
        "    return eval_loss.item(), pred_idx\n",
        "\n",
        "def predict(inputs, encoder, decoder, target_length=max_length):\n",
        "    ### Lúc dự đoán chúng ta cần tính kết quả ngay lập tức tại mỗi thời điểm, \n",
        "    ### rồi sau đó dừng từ được dự đoán để tính từ tiếp theo        \n",
        "    batch_size = inputs.size()[1]\n",
        "    \n",
        "    # input đầu tiên của mô hình decoder là kí tự bắt đầu, chúng ta dự đoán kí tự tiếp theo, sau đó lại dùng kí tự này để dự đoán từ kế tiếp\n",
        "    decoder_inputs = Variable(torch.ones((1, batch_size), dtype=torch.long)*sos_idx)\n",
        "    \n",
        "    # tính hidden state của mô hình encoder, cũng là vector biểu diễn của các từ, chúng ta cần tính context vector dựa trên những hidden states này\n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "    hidden = encoder_hidden\n",
        "    \n",
        "    preds = []\n",
        "    attn_weights = []\n",
        "    # chúng ta tính từng từ tại mỗi thời điểm\n",
        "    for i in range(target_length):\n",
        "        # dự đoán từ đầu tiên\n",
        "        output, hidden, attn_weight = decoder(decoder_inputs, hidden, encoder_outputs)\n",
        "        output = output.squeeze(dim=0)\n",
        "        pred_idx = torch.argmax(output, dim=-1)\n",
        "        \n",
        "        # thay đổi input tiếp theo bằng từ vừa được dự đoán\n",
        "        decoder_inputs = Variable(torch.ones((1, batch_size), dtype=torch.long)*pred_idx)\n",
        "        preds.append(decoder_inputs)\n",
        "        attn_weights.append(attn_weight.detach())\n",
        "    \n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    preds = torch.transpose(preds, 0, 1)\n",
        "    attn_weights = torch.cat(attn_weights, dim=0)\n",
        "    attn_weights = torch.transpose(attn_weights, 0, 1)\n",
        "    return preds, attn_weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki3OPE6CQmCG",
        "outputId": "1039efbb-c25c-407a-a1bd-d630ddf39bbc"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size)\n",
        "decoder = Decoder(output_size, hidden_size, 0.1)\n",
        "\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "X_val = torch.tensor(X_test, dtype=torch.long)\n",
        "y_val = torch.tensor(y_test, dtype=torch.long)\n",
        "X_val = torch.transpose(X_val, 0, 1)\n",
        "y_val = torch.transpose(y_val, 0, 1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for idx in range(len(X_train)//batch_size):\n",
        "        # input đầu vào của chúng ta là timestep first nhé. \n",
        "        X_train_batch = torch.tensor(X_train[batch_size*idx:batch_size*(idx+1)], dtype=torch.long)\n",
        "        y_train_batch = torch.tensor(y_train[batch_size*idx:batch_size*(idx+1)], dtype=torch.long)\n",
        "        \n",
        "        X_train_batch = torch.transpose(X_train_batch, 0, 1)\n",
        "        y_train_batch = torch.transpose(y_train_batch, 0, 1)\n",
        "        train_loss= train(X_train_batch, y_train_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    eval_loss, preds = evaluate(X_val, y_val, encoder, decoder, criterion)\n",
        "    \n",
        "    print('Epoch {} - train loss: {:.3f} - eval loss: {:.3f}'.format(epoch, train_loss, eval_loss))\n",
        "    print_idx = np.random.randint(0, len(preds), 3)\n",
        "    for i in print_idx:\n",
        "        x_val = decoder_sentence(X_val[:,i].numpy(), x_id2w)\n",
        "        y_pred = decoder_sentence(preds[i], y_id2w)\n",
        "        print(\" {:<35s}\\t{:>10}\".format(x_val, y_pred))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - train loss: 0.254 - eval loss: 0.255\n",
            " 03/11/2018                         \t2018-11-03\n",
            " 18/10/1975                         \t1975-10-18\n",
            " 24, thg 6 2015                     \t2015-06-24\n",
            "Epoch 1 - train loss: 0.033 - eval loss: 0.035\n",
            " thứ sáu, ngày 25 tháng 7 năm 1997  \t1997-07-25\n",
            " 16.06.72                           \t1972-06-16\n",
            " 07.08.15                           \t2015-08-07\n",
            "Epoch 2 - train loss: 0.014 - eval loss: 0.016\n",
            " 08 thg 2 2003                      \t2003-02-08\n",
            " 23 06 06                           \t2006-06-23\n",
            " 23 thg 7 1982                      \t1982-07-23\n",
            "Epoch 3 - train loss: 0.007 - eval loss: 0.009\n",
            " 15 thg 5, 1972                     \t1972-05-15\n",
            " 12 thg 11 1994                     \t1994-11-12\n",
            " thứ hai, ngày 29 tháng 8 năm 2005  \t2005-08-29\n",
            "Epoch 4 - train loss: 0.006 - eval loss: 0.007\n",
            " 21 tháng 5 2018                    \t2018-05-21\n",
            " ngày 06 tháng 10 năm 2010          \t2010-10-06\n",
            " 26 thg 3 1977                      \t1977-03-26\n",
            "Epoch 5 - train loss: 0.003 - eval loss: 0.004\n",
            " 06.09.82                           \t1982-09-06\n",
            " 5 12 12                            \t2112-12-05\n",
            " thứ ba, ngày 11 tháng 1 năm 1977   \t1977-01-11\n",
            "Epoch 6 - train loss: 0.002 - eval loss: 0.003\n",
            " tháng 9 3, 1989                    \t1989-09-03\n",
            " 15 thg 2 1992                      \t1992-02-15\n",
            " 4 tháng 6 1976                     \t1976-06-04\n",
            "Epoch 7 - train loss: 0.002 - eval loss: 0.002\n",
            " 27, thg 1 2003                     \t2003-01-27\n",
            " 20.04.95                           \t1995-04-20\n",
            " thứ năm, ngày 14 tháng 11 năm 1991 \t1991-11-14\n",
            "Epoch 8 - train loss: 0.002 - eval loss: 0.002\n",
            " tháng 12 17 1994                   \t1994-12-17\n",
            " 22 tháng 7 2016                    \t2016-07-22\n",
            " 16/11/1995                         \t1995-11-16\n",
            "Epoch 9 - train loss: 0.001 - eval loss: 0.002\n",
            " 18, thg 7 2015                     \t2015-07-18\n",
            " tháng 3 23 1974                    \t1974-03-23\n",
            " 10 tháng 7 2011                    \t2011-07-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-jeBrySR-CP"
      },
      "source": [
        "preds, attn_weights = predict(X_val ,encoder, decoder, target_length=10)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UEyOPakSFjO"
      },
      "source": [
        "def show_attention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticks(np.arange(len(input_sentence)))\n",
        "    ax.set_xticklabels(list(input_sentence), rotation=90)\n",
        "    ax.set_yticks(np.arange(len(output_words)))\n",
        "    ax.set_yticklabels(list(output_words))\n",
        "    ax.grid()\n",
        "    ax.set_xlabel('Input Sequence')\n",
        "    ax.set_ylabel('Output Sequence')\n",
        "    plt.show()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "4w2PBPioSI1M",
        "outputId": "dff9f282-d645-4fb5-876e-bc8076382ab6"
      },
      "source": [
        "show_idx = randint(0, len(preds))\n",
        "text_x = decoder_sentence(X_val[:,show_idx].numpy(), x_id2w)\n",
        "text_y = decoder_sentence(preds[show_idx].numpy(), y_id2w)\n",
        "attn_weight = attn_weights[show_idx, :, -len(text_x):]\n",
        "show_attention(text_x, text_y, attn_weight)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}